{{[[((static domain))]]}}:

they are not globally unique; but locally they are. 
their unique zone is 10,000, their connections to their downstream are 
    numbers of downstream neurons. 
["to",345,567,223,667,234,543,234,654,64756,856,857,857,9999]
  vs.: {"to":[345,567,223,667,234,543,234,654,64756,856,857,857,9999]}?
  vs.:("to",345,567,223,667,234,543,234,654,64756,856,857,857,9999)?
each output is simplified to only [0,1] ? confirm?
    in fact the strength of each input could vary...
      can this be reflected in weights along?
        actually quite the reverse.
    decision: {{weight}} is an overall evaluation of rection strength,
              it's the summary effect of 'input strength', 'receiving strength'
their outputs have durations: time
  outputs from same neuron have the same duration (universally.)
their targets:
  have weight for each input onto themselvs: weight
  don't have collective information of who have input to themselvs.
  !? these two rules just conflict each other
      so the previous layer should store the weights to each input
      ["to",(345,-3),(567,1),(223,0),(667,4),(234,4),543,234,654]

!!! downer doesnot know the upper; if thereis feedback, its done by thirder [the third neuron in the loop]


pending check points:
  each value should be easily retrived by its name or location number
      also by its [local connection index] to next level.
        e.g: 
        this is to enable feedback to be delivered to the right upper neuron. 
        how?...


{{[[((functional domain))]]}}:

define a function that get called whenever something happens and can chnage neuronss values.
    use recursive functions. 
    dont bind any of them onto neurons themselvs
    so it means, separate the algo from static information? confirm? 
      observation: often times algo is info...




(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((())))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))